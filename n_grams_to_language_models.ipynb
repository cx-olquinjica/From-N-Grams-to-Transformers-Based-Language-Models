{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55a80671",
   "metadata": {},
   "source": [
    "# From N-Grams to Transformer-Based Language Models: Theoretical Foundation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ece748",
   "metadata": {},
   "source": [
    "When talking about language models, we are talking about models that assign probabilities to a sequence of tokens: \n",
    "$$\n",
    "p(w_1, w_2, \\ldots, w_M ), \\text{ with } w_m \\in V. \\text{ The set } V \\text{ is a discrete vocabulary,}\n",
    "V = \\{\\text{aardvark, abacus, } \\ldots, \\text{ zither}\\}.\n",
    "$$\n",
    "\n",
    "So one might ask why would you want to compute the probability of a word sequence? Well, in many applications the goal is to produce word sequences as output: \n",
    "\n",
    "- Machine Translation\n",
    "- Speech Recognition\n",
    "- Summarization\n",
    "- Dialogue Systems (Chat bots), etc\n",
    "\n",
    "In many of the systems for performing these tasks, there is a subcomponent that compputes the probability of the output text. The purpose of this component is to generate texts that are more fluent. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1ca135",
   "metadata": {},
   "source": [
    "## 1. Naive Approach: Unbiasead Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba3e5f9",
   "metadata": {},
   "source": [
    "Let's talk about a very __naive approach to build a language model__. A very naive language model would be one, for example that explores the concept of __relative frequency estimate__ to compute the probability of a sequence of tokens. \n",
    "\n",
    "Let's work through an example to see this concept in action. Consider the sentence: \"Computers are useless, they can only give you answers.\" Now let's estimate the probability of this sequence of word tokens using the relative frequency estimate: \n",
    "\n",
    "$$\n",
    "p(\\text{Computers are useless, they can only give you answers})\n",
    "= \\frac{\\text{count}(\\text{Computers are useless, they can only give you answers}) \\ }{\\text{count}(\\text{all sentences ever spoken})}\n",
    "$$\n",
    "\n",
    "This way of modeling language has a very serious problem, in the theoretical limit of infinite data it would be indeed a good solution, however it is very hard to estimate the count of all the sentences ever spoken, one cannot even imagine how big the dataset would have to be to have an accurate count of all the sentences ever spoken, because of this phenomenon this estimator is said to be __unbiased__. \n",
    "\n",
    "Another thing to notice about this implementation is that even grammatically correct sentences would have very low probabilities if they are not included in the set of _all sentences ever spoken_ (i.e., in case we can group them in a dataset). Clearly, this estimator is very data-hungry, and suffers from high vari- ance: even grammatical sentences will have probability zero if they have not occurred in the training data(_side note_:Chomsky famously argued that this is evidence against the very concept of probabilistic language mod- els: no such model could distinguish the grammatical sentence colorless green ideas sleep furiously from the ungrammatical permutation furiously sleep ideas green colorless.)\n",
    "\n",
    "Therefore what is the solution to this problem? And how can we solve it, __we need to  to introduce bias to have a chance of making reliable estimates from finite training data__. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9165a3",
   "metadata": {},
   "source": [
    "## 2. N-grams Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb598bb8",
   "metadata": {},
   "source": [
    "The n-gram language model on the other hand __computes the probability of sequence of tokens as the product of probability of subsequences__. \n",
    "\n",
    "$$\n",
    "\\text{The probability of a sequence } p(\\mathbf{w}) = p(w_1, w_2, \\ldots, w_M) \\text{ can be refactored using the chain rule}\n",
    "$$\n",
    "\n",
    "$$\n",
    "p(\\mathbf{w}) = p(w_1, w_2, \\ldots, w_M) \\quad \\text{} = p(w_1) \\times p(w_2 \\mid w_1) \\times p(w_3 \\mid w_2, w_1) \\times \\ldots \\times p(w_M \\mid w_{M-1}, w_1) \\quad \\text{}\n",
    "$$\n",
    "\n",
    "\n",
    "Each element in the product is the probability of a word given all its predecessors. We can think of this as a _word prediction task_: given the context _Computers are_, we want to compute a probability over the next token. The relative frequency estimate of the probability of the word _useless_ in this context is,\n",
    "\n",
    "\n",
    "$$\n",
    "p(\\text{useless} \\mid \\text{computers are}) = \\frac{\\text{count(computers are useless } )}{\\sum_{x \\in V} \\text{count(computers are } x)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\frac{\\text{count(computers are useless)}}{\\text{count(computers are)}}\n",
    "$$\n",
    "\n",
    "If you think carefully about the denominator you can see that we haven't really made any progress so far. To computer the conditional probability $$\n",
    "p(w_M \\mid w_{M-1}, w_{M-2}, \\ldots, w_1)\n",
    "$$\n",
    " \n",
    "we would need to model $$\n",
    "V^{M-1}\n",
    "$$ contexts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270150a1",
   "metadata": {},
   "source": [
    "To solve this problem n-grams models make a very interesting assumption, __they condition only on the past n-1 words__: \n",
    "\n",
    "$$\n",
    "p(w_m \\mid w_{m-1}, \\ldots, w_1) \\approx p(w_m \\mid w_{m-1}, \\ldots, w_{m-n+1})\n",
    "$$\n",
    "\n",
    "This means that the probability of a sentence can be approximate as: \n",
    "\n",
    "$$\n",
    "\\text{This model requires estimating and storing the probability of only } V^n \\text{ events, which is exponential in the order of the n-gram, and not } V^M, \\text{ which is exponential in the length of the sentence.}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{The n-gram probabilities can be computed by relative frequency estimation,}\n",
    "$$\n",
    "\n",
    "$$\n",
    "p(w_m \\mid w_{m-1}, w_{m-2}) = \\frac{\\text{count}(w_{m-2}, w_{m-1}, w_m)}{\\sum_{w'} \\text{count}(w_{m-2}, w_{m-1}, w')}\n",
    "\\quad \\text{[6.12]}\n",
    "$$\n",
    "\n",
    "The hyperparameter  _n_  controls the size of the context used in each conditional probability. If this is misspecified, the language model will perform poorly. Let’s consider the potential problems concretely.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0bab6d",
   "metadata": {},
   "source": [
    "- When n is too small. Consider the following sentences:\n",
    "  - __Gorillas__ always like to groom their __friends__.\n",
    "  - The __computer__ that’s on the 3rd floor of our office building __crashed__.\n",
    "\n",
    "In each example, the words written in bold depend on each other: the likelihood of __their__ depends on knowing that __gorillas__ is plural, and the likelihood of __crashed__ depends on knowing that the subject is a __computer__. _If the n-grams are not big enough to capture this context, then the resulting language model would offer probabilities that are too low for these sentences, and too high for sentences that fail basic linguistic tests like number agreement_.\n",
    "- When n is too big.\n",
    "In this case, it is hard good estimates of the n-gram parameters from our dataset, because of data sparsity. To handle the gorilla example, it is necessary to model 6-grams, which means accounting for V 6 events. Under a very small vocab- ulary of V = 104, this means estimating the probability of 1024 distinct events."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0538b18",
   "metadata": {},
   "source": [
    "These two problems point to another bias-variance tradeoff (see § 2.2.4). A small n- gram size introduces high bias, and a large n-gram size introduces high variance. We can even have both problems at the same time! Language is full of long-range dependen- cies that we cannot capture because n is too small; at the same time, language datasets are full of rare phenomena, whose probabilities we fail to estimate accurately because n is too large. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f34181",
   "metadata": {},
   "source": [
    "#### One solution is to try to keep _n_ large, while still making low-variance estimates of the underlying parameters. To do this, we will introduce a different sort of bias: smoothing."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
